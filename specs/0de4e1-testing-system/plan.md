---
runId: 0de4e1
feature: testing-system
created: 2025-11-03
status: ready
---

# Testing System - Implementation Plan

> **Generated by:** Task Decomposition skill
> **From spec:** specs/0de4e1-testing-system/spec.md
> **Created:** 2025-11-03

## Execution Summary

- **Total Tasks**: 5
- **Total Phases**: 3
- **Sequential Time**: 19h
- **Parallel Time**: 11h
- **Time Savings**: 8h (42%)

**Parallel Opportunities:**

- Phase 1: 3 tasks (8h saved - 57% faster)

---

## Phase 1: Foundation

**Strategy**: Parallel
**Reason**: All tasks are independent with no shared files. Test fixtures, scenarios, and skill can be created simultaneously.

### Task 1: Test Fixtures

**Files**:

- tests/fixtures/simple-typescript/CLAUDE.md
- tests/fixtures/simple-typescript/package.json
- tests/fixtures/simple-typescript/tsconfig.json
- tests/fixtures/simple-typescript/src/index.ts
- tests/fixtures/simple-typescript/.gitignore
- tests/fixtures/simple-python/CLAUDE.md
- tests/fixtures/simple-python/requirements.txt
- tests/fixtures/simple-python/src/main.py
- tests/fixtures/simple-python/.gitignore

**Complexity**: M (4h)

**Dependencies**: None

**Description**:

Create two minimal project fixtures for testing spectacular commands end-to-end. Each fixture must be a valid git repository with git-spice initialized and setup commands defined in CLAUDE.md.

**Implementation Steps**:

1. Create `tests/fixtures/simple-typescript/` directory structure
2. Add package.json with minimal dependencies (typescript, @types/node)
3. Add tsconfig.json with basic configuration
4. Add src/index.ts with trivial "Hello World" function
5. Add CLAUDE.md with setup commands (install: `npm install`, test: `npm test`, lint: `npm run lint`, build: `npm run build`)
6. Add .gitignore (node_modules, dist)
7. Initialize git repo: `cd tests/fixtures/simple-typescript && git init && git add . && git commit -m "Initial commit"`
8. Initialize git-spice: `cd tests/fixtures/simple-typescript && gs repo init`
9. Create `tests/fixtures/simple-python/` directory structure
10. Add requirements.txt with minimal dependencies (pytest)
11. Add src/main.py with trivial "Hello World" function
12. Add CLAUDE.md with setup commands (install: `pip install -r requirements.txt`, test: `pytest`, lint: `ruff check .`)
13. Add .gitignore (__pycache__, *.pyc, .pytest_cache)
14. Initialize git repo: `cd tests/fixtures/simple-python && git init && git add . && git commit -m "Initial commit"`
15. Initialize git-spice: `cd tests/fixtures/simple-python && gs repo init`
16. Verify both fixtures can be cloned and setup in <1 minute

**Acceptance Criteria**:

- [ ] `tests/fixtures/simple-typescript/` is valid git repo with git-spice initialized
- [ ] `tests/fixtures/simple-python/` is valid git repo with git-spice initialized
- [ ] Each fixture has CLAUDE.md with setup commands (install, test, lint, build/format)
- [ ] Fixtures can be cloned and setup in <1 minute (test by timing setup process)

**Mandatory Patterns**:

> **Constitution**: All code must follow @docs/constitutions/current/

See patterns.md for file structure conventions and testing.md for fixture requirements.

**TDD**: Not applicable (no code to test - creating project templates)

**Quality Gates**:

```bash
# Validate git initialization
cd tests/fixtures/simple-typescript && git status
cd tests/fixtures/simple-python && git status

# Validate git-spice initialization
cd tests/fixtures/simple-typescript && gs ls
cd tests/fixtures/simple-python && gs ls

# Validate setup speed
time (cd tests/fixtures/simple-typescript && npm install)
time (cd tests/fixtures/simple-python && pip install -r requirements.txt)
```

---

### Task 2: Test Scenarios

**Files**:

- tests/scenarios/execute/parallel-stacking-2-tasks.md
- tests/scenarios/execute/parallel-stacking-3-tasks.md
- tests/scenarios/execute/parallel-stacking-4-tasks.md
- tests/scenarios/execute/sequential-stacking.md
- tests/scenarios/execute/worktree-creation.md
- tests/scenarios/execute/cleanup-tmp-branches.md
- tests/scenarios/init/validate-superpowers.md
- tests/scenarios/init/validate-git-spice.md
- tests/scenarios/init/error-handling.md
- tests/scenarios/spec/lean-spec-generation.md
- tests/scenarios/plan/task-decomposition.md

**Complexity**: L (6h)

**Dependencies**: None

**Description**:

Create 11 test scenario markdown files that document expected behavior, failure modes, and success criteria for testing spectacular commands. Focus on execute command scenarios that would have caught the 9f92a8 stacking regression.

**Implementation Steps**:

1. Create `tests/scenarios/` directory structure with subdirectories for each command
2. Write `parallel-stacking-2-tasks.md` (baseline case - simplest parallel execution)
3. Write `parallel-stacking-3-tasks.md` (common case - moderate parallelism)
4. Write `parallel-stacking-4-tasks.md` (edge case that caught worktree creation flaw)
5. Write `sequential-stacking.md` (no parallelism - linear stack)
6. Write `worktree-creation.md` (verify worktree setup and cleanup)
7. Write `cleanup-tmp-branches.md` (verify tmp branches removed after cleanup)
8. Write `validate-superpowers.md` (init command checks for superpowers plugin)
9. Write `validate-git-spice.md` (init command checks for git-spice CLI)
10. Write `error-handling.md` (init command handles missing dependencies gracefully)
11. Write `lean-spec-generation.md` (spec command produces lean spec with constitution references)
12. Write `task-decomposition.md` (plan command decomposes spec into phases)
13. Each scenario must include: Context, Expected Behavior, Failure Modes, Success Criteria sections
14. Reference specific failure modes from 9f92a8 regression analysis
15. Include rationalization examples (e.g., "manually tested once is enough")

**Acceptance Criteria**:

- [ ] All 11 test scenarios have standard structure (Context, Expected Behavior, Failure Modes, Success Criteria)
- [ ] Execute command has 6 scenarios covering parallel stacking edge cases (2-task, 3-task, 4-task)
- [ ] Scenarios reference specific failure modes from 9f92a8 regression (worktree creation timing, tmp branch cleanup)
- [ ] Each scenario is executable by subagents via `testing-workflows-with-subagents` skill

**Mandatory Patterns**:

> **Constitution**: All code must follow @docs/constitutions/current/

See patterns.md for markdown structure conventions and testing.md for scenario documentation patterns.

**TDD**: Not applicable (documentation files, not code)

**Quality Gates**:

```bash
# Validate all scenarios exist
ls tests/scenarios/execute/ | wc -l  # Should be 6
ls tests/scenarios/init/ | wc -l     # Should be 3
ls tests/scenarios/spec/ | wc -l     # Should be 1
ls tests/scenarios/plan/ | wc -l     # Should be 1

# Validate structure (each file has required sections)
grep -l "## Context" tests/scenarios/**/*.md | wc -l  # Should be 11
grep -l "## Expected Behavior" tests/scenarios/**/*.md | wc -l  # Should be 11
grep -l "## Failure Modes" tests/scenarios/**/*.md | wc -l  # Should be 11
grep -l "## Success Criteria" tests/scenarios/**/*.md | wc -l  # Should be 11
```

---

### Task 3: Testing Skill

**Files**:

- skills/testing-spectacular/SKILL.md

**Complexity**: M (4h)

**Dependencies**: None

**Description**:

Create a reusable skill that documents the testing process for spectacular commands and workflows following superpowers format. Use `writing-skills` metaskill from superpowers to ensure skill is bulletproof.

**Implementation Steps**:

1. Use `writing-skills` metaskill from superpowers to scaffold skill structure
2. Write "When to Use" section (before releases, after finding regressions, when editing commands/skills)
3. Write "The Process" section following RED-GREEN-REFACTOR methodology:
   - RED: Run baseline test without fixes (document failures)
   - GREEN: Apply fixes to commands/skills
   - REFACTOR: Re-run tests with edge cases to catch flaws
   - Iterate until all scenarios pass
4. Write "Quality Rules" section:
   - All scenarios must pass before committing fixes
   - Test scenarios must be executable by subagents
   - Edge cases must be tested (not just happy path)
   - Regression tests must be added when bugs are found
5. Write "Rationalization Table" with shortcuts observed during 9f92a8 testing:
   - "Manually tested once is enough" → Must run full test suite
   - "This scenario is unlikely" → Must test edge cases
   - "I'll add tests later" → Tests must be added with fix
6. Add TodoWrite checklist for RED-GREEN-REFACTOR phases
7. Document integration with `testing-workflows-with-subagents` from superpowers
8. Include error handling section (test fixtures missing, scenarios malformed)
9. Test skill itself using `testing-skills-with-subagents` before completing task

**Acceptance Criteria**:

- [ ] `skills/testing-spectacular/SKILL.md` follows superpowers format (name, description frontmatter)
- [ ] Skill includes rationalization table with shortcuts from 9f92a8 testing experience
- [ ] Skill requires TodoWrite checklist for RED-GREEN-REFACTOR phases
- [ ] Skill has been tested with `testing-skills-with-subagents` before deployment (evidence required)

**Mandatory Patterns**:

> **Constitution**: All code must follow @docs/constitutions/current/

See patterns.md for skill documentation patterns and testing.md for RED-GREEN-REFACTOR methodology.

**TDD**: Use `testing-skills-with-subagents` to test the skill itself before deployment.

**Quality Gates**:

```bash
# Validate frontmatter
grep -A 2 "^---" skills/testing-spectacular/SKILL.md | grep "name: testing-spectacular"
grep -A 3 "^---" skills/testing-spectacular/SKILL.md | grep "description:"

# Validate required sections
grep "## When to Use" skills/testing-spectacular/SKILL.md
grep "## The Process" skills/testing-spectacular/SKILL.md
grep "## Quality Rules" skills/testing-spectacular/SKILL.md
grep "## Rationalization Table" skills/testing-spectacular/SKILL.md
grep "## Error Handling" skills/testing-spectacular/SKILL.md

# Must use testing-skills-with-subagents before completing
# (Will be verified in PR - orchestrator must see evidence of skill testing)
```

---

## Phase 2: Integration

**Strategy**: Sequential
**Reason**: Test command depends on all Phase 1 components (fixtures to clone, scenarios to run, skill to reference).

### Task 4: Test Command

**Files**:

- commands/test.md

**Complexity**: M (3h)

**Dependencies**: task-1-test-fixtures, task-2-test-scenarios, task-3-testing-skill

**Description**:

Create `/spectacular:test [command]` command that orchestrates test execution by dispatching subagents in parallel to run test scenarios from `tests/scenarios/`. Command must clone fixtures, run scenarios, aggregate results, and report pass/fail.

**Implementation Steps**:

1. Create `commands/test.md` with YAML frontmatter (description: "Execute test scenarios for spectacular commands")
2. Parse command argument to determine which scenarios to run (execute, init, spec, plan, all)
3. If "all", include all scenarios from all command subdirectories
4. Clone appropriate test fixture to temp directory (`.worktrees/test-{timestamp}/`)
5. Dispatch subagents in parallel using Task tool (one subagent per scenario)
6. Each subagent instruction must:
   - Change to test fixture directory
   - Load scenario from `tests/scenarios/{command}/{scenario}.md`
   - Execute scenario using `testing-workflows-with-subagents` skill
   - Report pass/fail with evidence
7. Aggregate results from all subagents
8. Report summary: X/Y scenarios passed
9. Clean up temp directories (`rm -rf .worktrees/test-*`)
10. Exit with non-zero code if any test fails (for future CI integration)
11. Include error handling for missing scenarios, fixture clone failures, subagent failures

**Acceptance Criteria**:

- [ ] `/spectacular:test [command]` dispatches subagents to run scenarios in parallel
- [ ] Each subagent gets fresh test fixture clone (isolated execution)
- [ ] Test results aggregated and reported at end (X/Y passed)
- [ ] Exit code non-zero if any test fails (verified by running with intentional failure)

**Mandatory Patterns**:

> **Constitution**: All code must follow @docs/constitutions/current/

See patterns.md for command orchestration patterns and testing.md for subagent dispatch patterns.

**TDD**: Follow `test-driven-development` skill. Write test scenario for the test command itself (meta-testing), watch it fail, implement command, watch it pass.

**Quality Gates**:

```bash
# Validate command frontmatter
grep -A 2 "^---" commands/test.md | grep "description:"

# Test command execution (must work)
/spectacular:test execute  # Should dispatch subagents

# Test error handling
/spectacular:test nonexistent  # Should report "no scenarios found for 'nonexistent'"
```

---

## Phase 3: Documentation

**Strategy**: Sequential
**Reason**: Documentation depends on all components being complete (Task 4 completion).

### Task 5: Documentation

**Files**:

- TESTING.md

**Complexity**: S (2h)

**Dependencies**: task-4-test-command

**Description**:

Create TESTING.md that documents how to use the testing system, add new scenarios, and integrate testing into development workflow.

**Implementation Steps**:

1. Create `TESTING.md` at repository root
2. Write "Overview" section explaining testing system purpose
3. Write "Running Tests" section with examples:
   - `/spectacular:test all` (full suite)
   - `/spectacular:test execute` (specific command)
   - How to interpret results
4. Write "Adding Test Scenarios" section:
   - Create new file in `tests/scenarios/{command}/`
   - Follow standard structure (Context, Expected Behavior, Failure Modes, Success Criteria)
   - Example scenario walkthrough
5. Write "Test Fixtures" section:
   - What fixtures exist (simple-typescript, simple-python)
   - When to use each fixture
   - How to add new fixtures (rare - only if testing language-specific behavior)
6. Write "Testing Workflow" section:
   - Before releases: Run `/spectacular:test all`
   - After finding regressions: Add scenario + fix + re-test (RED-GREEN-REFACTOR)
   - When editing commands/skills: Run relevant scenarios
7. Link to `skills/testing-spectacular/SKILL.md` for detailed methodology
8. Link to constitution testing.md for broader testing philosophy
9. Include troubleshooting section (fixtures missing, git-spice not initialized)

**Acceptance Criteria**:

- [ ] TESTING.md exists and documents how to use `/spectacular:test` command
- [ ] Examples of adding new test scenarios with standard structure
- [ ] Guidance on when to run tests (before releases, after finding bugs, when editing)

**Mandatory Patterns**:

> **Constitution**: All code must follow @docs/constitutions/current/

See patterns.md for documentation conventions.

**TDD**: Not applicable (documentation file)

**Quality Gates**:

```bash
# Validate documentation completeness
grep "## Overview" TESTING.md
grep "## Running Tests" TESTING.md
grep "## Adding Test Scenarios" TESTING.md
grep "## Test Fixtures" TESTING.md
grep "## Testing Workflow" TESTING.md
grep "## Troubleshooting" TESTING.md

# Validate links
grep "skills/testing-spectacular/SKILL.md" TESTING.md
grep "docs/constitutions/current/testing.md" TESTING.md
```

---

## Execution Instructions

**For `/spectacular:execute` command:**

1. **Phase 1 (Parallel)**: Dispatch 3 subagents simultaneously
   - Subagent A: Task 1 (Test Fixtures)
   - Subagent B: Task 2 (Test Scenarios)
   - Subagent C: Task 3 (Testing Skill)

2. **Phase 2 (Sequential)**: After Phase 1 completes, dispatch 1 subagent
   - Subagent D: Task 4 (Test Command)

3. **Phase 3 (Sequential)**: After Phase 2 completes, dispatch 1 subagent
   - Subagent E: Task 5 (Documentation)

**Quality Gates After Each Phase:**

- Run `git status` to verify files created
- Run relevant validation commands from task acceptance criteria
- Use `requesting-code-review` skill after Phase 2 and Phase 3

**Completion Criteria:**

- All 5 tasks completed
- All acceptance criteria verified
- `/spectacular:test all` runs successfully (Phase 2+ only)
- Use `verification-before-completion` skill to verify all tests pass before claiming completion
