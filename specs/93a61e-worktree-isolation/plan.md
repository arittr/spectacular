---
runId: 93a61e
feature: worktree-isolation
created: 2025-10-28
status: ready
---

# Feature: Worktree-Based Execution Isolation - Implementation Plan

> **Generated by:** Task Decomposition skill
> **From spec:** specs/93a61e-worktree-isolation/spec.md
> **Created:** 2025-10-28

## Execution Summary

- **Total Tasks**: 5
- **Total Phases**: 3
- **Sequential Time**: 26h
- **Parallel Time**: 14h
- **Time Savings**: 12h (46%)

**Parallel Opportunities:**

- Phase 2: 3 tasks (9h saved - spec, plan, execute commands modified independently)
- Phase 3: 2 tasks (3h saved - list and cleanup commands created independently)

**Advanced**: Phases 2 & 3 could run simultaneously after Phase 1 (total: 11h, 58% savings)

---

## Phase 1: Foundation

**Strategy**: Sequential
**Reason**: managing-main-worktrees skill must exist before commands can use it

### Task 1-1: Main Worktree Management Skill

**Files**:

- `skills/managing-main-worktrees/SKILL.md`

**Complexity**: M (4h)

**Dependencies**: None

**Description**:
Create the `managing-main-worktrees` skill that encapsulates all main worktree lifecycle operations (check/create/recover). This skill will be used by all commands (spec, plan, execute, list, cleanup) to ensure consistent worktree handling.

**Why this chunking:**

- Complete, reusable abstraction for worktree operations
- Single source of truth for worktree patterns
- Testable in isolation before commands use it
- Natural boundary: skill is foundation for all command modifications

**Implementation Steps**:

1. Create `skills/managing-main-worktrees/SKILL.md` with frontmatter
2. Document skill purpose: "Use when commands need to check/create/recover main worktrees for feature isolation"
3. Implement check logic:
   - Verify worktree exists: `git worktree list | grep ".worktrees/main-{runId}"`
   - Return exists/missing status
4. Implement create logic:
   - Run `git worktree add .worktrees/main-{runId} --detach HEAD`
   - Verify creation succeeded
   - Document detached HEAD state (intentional, allows independent branches)
5. Implement recover logic:
   - If operations fail: run `git worktree prune`
   - Retry operation once
   - If still fails: error with clear message
6. Document anti-patterns:
   - NEVER cd/pwd to verify worktree (commands cd after skill completes)
   - NEVER run prune proactively (only as recovery)
7. Document error handling:
   - Worktree already exists: reuse it (not an error)
   - Worktree corrupted: prune and recreate
   - Disk space issues: clear error message
8. Add quality checklist and rationalization table
9. Test skill independently with manual worktree operations

**Acceptance Criteria**:

- [ ] Skill file exists with proper frontmatter (name, description)
- [ ] Check logic correctly detects existing worktrees
- [ ] Create logic creates worktrees in detached HEAD state
- [ ] Recover logic uses prune only when operations fail
- [ ] Skill does not cd/pwd (commands handle that)
- [ ] Error messages are clear and actionable
- [ ] Skill can be invoked with `Skill` tool
- [ ] Manual testing: create, check, recover workflows succeed

**Mandatory Patterns**:

> **Constitution**: All code must follow @docs/constitutions/current/

See patterns.md for skill documentation standards.

**TDD**: Not applicable (markdown skill documentation, no code)

**Quality Gates**:

```bash
# Verify skill loads
# Manual test: use skill in test command
```

---

## Phase 2: Core Command Modifications

**Strategy**: Parallel
**Reason**: Commands modify different files, no shared dependencies. All depend on Phase 1 skill.

### Task 2-1: Spec Command Worktree Integration

**Files**:

- `commands/spec.md`

**Complexity**: L (6h)

**Dependencies**: task-1-1 (needs managing-main-worktrees skill)

**Description**:
Modify spec command to create main worktree, handle uncommitted changes (4 options), dispatch subagent for spec generation with constitution pre-loading, and keep orchestrator in main repo.

**Why this chunking:**

- Complete spec command workflow from start to finish
- Single PR for all spec-related worktree behavior
- Includes uncommitted changes handling (complex but cohesive)
- Natural boundary: one command, one task

**Implementation Steps**:

1. Read current `commands/spec.md` to understand existing workflow
2. Add Step 0 after RUN_ID generation: feature slug generation
   - Implement slug rules: lowercase, hyphens, no special chars, 50 char max
   - Add examples to command documentation
3. Add Step 0.5: Uncommitted changes handling
   - Check `git status --porcelain`
   - If dirty: implement AskUserQuestion with 4 options:
     - Option 1: Commit with "WIP: Spectacular spec creation"
       - Handle pre-commit hook failures (show output, offer retry/abort)
     - Option 2: Stash with `git stash push -m "Spectacular: {feature-slug}"`
       - Output stash reference
       - Handle stash failures (offer proceed anyway/abort)
     - Option 3: Proceed anyway (uncommitted stay in main repo)
     - Option 4: Abort
   - Document consequences for each option clearly
4. Add main worktree creation using `managing-main-worktrees` skill
   - Skill invocation: check/create `.worktrees/main-{runId}/`
   - Orchestrator remains in main repo (does NOT cd)
5. Modify spec generation to use subagent dispatch (Task tool)
   - Pass working directory path: `.worktrees/main-{runId}/`
   - Pass runId and feature description
   - Subagent prompt pre-loads constitution context:
     - Read @docs/constitutions/current/architecture.md
     - Read @docs/constitutions/current/patterns.md
     - Read other constitutions as needed
   - Subagent workflow:
     - cd to working directory first
     - Use `superpowers:brainstorming` skill (constitution-aware)
     - Use `spectacular:writing-specs` skill to formalize spec
     - Write spec to `specs/{runId}-{feature-slug}/spec.md` (relative path)
     - Return summary to orchestrator
6. Update completion reporting:
   - Report absolute path: `.worktrees/main-{runId}/specs/...`
   - Verify main repo has no uncommitted files
7. Document NFR6 rationale (context management, no compaction)
8. Test command end-to-end with clean and dirty repo states

**Acceptance Criteria**:

- [ ] Feature slug generated correctly (S1, S7 test scenarios)
- [ ] Uncommitted changes: 4 options presented (S2-S5 scenarios)
- [ ] Commit option with hook failure handling (S6 scenario)
- [ ] Worktree created using managing-main-worktrees skill
- [ ] Orchestrator stays in main repo (verify with pwd)
- [ ] Subagent dispatched with constitution pre-loading (S8 scenario)
- [ ] Subagent uses superpowers:brainstorming and spectacular:writing-specs
- [ ] Spec created at `.worktrees/main-{runId}/specs/{runId}-{feature-slug}/spec.md`
- [ ] Absolute path reported to user
- [ ] Main repo clean after spec generation

**Mandatory Patterns**:

> **Constitution**: All code must follow @docs/constitutions/current/

Command documentation follows patterns.md standards.

**TDD**: Use test-scenarios.md sections S1-S8 for validation

**Quality Gates**:

```bash
# Test spec command with clean repo
/spectacular:spec "Test feature"

# Test with uncommitted changes
echo "test" >> README.md
/spectacular:spec "Another feature"

# Verify worktree isolation
git status --porcelain  # Should be empty in main repo
ls .worktrees/main-*/
```

---

### Task 2-2: Plan Command Worktree Integration

**Files**:

- `commands/plan.md`

**Complexity**: M (3h)

**Dependencies**: task-1-1 (needs managing-main-worktrees skill)

**Description**:
Modify plan command to extract runId from spec path, use managing-main-worktrees skill to check/create worktree, cd to worktree for plan generation, and report absolute path.

**Why this chunking:**

- Complete plan command workflow
- Simpler than spec (no subagent, no uncommitted handling)
- Single PR for plan-related worktree behavior
- Natural boundary: one command, one task

**Implementation Steps**:

1. Read current `commands/plan.md` to understand existing workflow
2. Add Step 0: Main worktree setup
   - Extract runId from spec path using regex: `specs/([^-]+)-`
   - If extraction fails: error with expected format message
   - Use `managing-main-worktrees` skill: check/create `.worktrees/main-{runId}/`
   - Orchestrator cd's to worktree: `cd .worktrees/main-{runId}/`
3. Update task decomposition to work from worktree
   - Plan written to `specs/{runId}-{feature-slug}/plan.md` (relative path)
   - All file paths in plan remain relative to worktree
4. Update completion reporting:
   - Report absolute path: `.worktrees/main-{runId}/specs/...`
   - Verify main repo has no uncommitted files
5. Document that plan generation runs in orchestrator (must cd)
6. Test command with existing and non-existing worktrees

**Acceptance Criteria**:

- [ ] RunId extracted correctly from spec path (P1, P3 scenarios)
- [ ] Invalid path format shows clear error (P3 scenario)
- [ ] Worktree created/reused using managing-main-worktrees skill (P2 scenario)
- [ ] Orchestrator cd's to worktree (verify with pwd)
- [ ] Plan created at `.worktrees/main-{runId}/specs/{runId}-{feature-slug}/plan.md`
- [ ] Absolute path reported to user
- [ ] Main repo clean after plan generation

**Mandatory Patterns**:

> **Constitution**: All code must follow @docs/constitutions/current/

Command documentation follows patterns.md standards.

**TDD**: Use test-scenarios.md sections P1-P3 for validation

**Quality Gates**:

```bash
# Test plan command
/spectacular:plan @specs/93a61e-worktree-isolation/spec.md

# Verify worktree isolation
git status --porcelain  # Should be empty in main repo
cat .worktrees/main-93a61e/specs/93a61e-worktree-isolation/plan.md
```

---

### Task 2-3: Execute Command Worktree Integration

**Files**:

- `commands/execute.md`

**Complexity**: L (7h)

**Dependencies**: task-1-1 (needs managing-main-worktrees skill)

**Description**:
Modify execute command to use managing-main-worktrees skill, implement dependency detection (CLAUDE.md → constitution → LLM inference), cd to worktree for install/coordination, and pass working directory paths to subagents.

**Why this chunking:**

- Complete execute command workflow
- Includes complex dependency detection (but cohesive unit)
- Single PR for execute-related worktree behavior
- Natural boundary: one command, one task

**Implementation Steps**:

1. Read current `commands/execute.md` to understand existing workflow
2. Add Step 0.5: Main worktree setup
   - Extract runId from plan path using regex: `specs/([^-]+)-`
   - Use `managing-main-worktrees` skill: check/create `.worktrees/main-{runId}/`
   - Orchestrator cd's to worktree: `cd .worktrees/main-{runId}/`
3. Add Step 0.6: Dependency detection and installation
   - Priority 1: Read CLAUDE.md, look for install/setup/dependencies sections
   - Priority 2: Read @docs/constitutions/current/tech-stack.md
   - Priority 3: LLM inference on root directory files
     - Examine lock files, config files, build manifests
     - Infer language/ecosystem from patterns (package.json+lock → Node.js, etc.)
     - Select appropriate install command for detected ecosystem
     - Prioritize faster package managers (pnpm > npm, uv > pip)
   - Priority 4: Skip install (log info message, not error)
   - Run detected install command in worktree (orchestrator executes)
   - If install fails: fail entire execution with error message
4. Update Phase 2 execution to pass working directory paths:
   - Sequential task subagents: receive `.worktrees/main-{runId}/`, cd themselves
   - Parallel setup subagent: receives working directory path, cd's to create child worktrees
   - Parallel task subagents: receive child worktree paths `.worktrees/{runId}-task-X-Y/`, cd themselves
   - Continue using `using-git-worktrees` skill for parallel task worktrees
5. Document that orchestrator cd's but subagents receive paths explicitly
6. Test command with various ecosystems (Node.js, Python, Rust, Go)
7. Test install failures and graceful skipping

**Acceptance Criteria**:

- [ ] Worktree created/reused using managing-main-worktrees skill
- [ ] Orchestrator cd's to worktree (verify with pwd)
- [ ] Dependency detection: CLAUDE.md priority (E2 scenario)
- [ ] Dependency detection: constitution fallback (E3 scenario)
- [ ] Dependency detection: LLM inference for multiple ecosystems (E4 scenario)
- [ ] Dependency detection: graceful skip (E5 scenario)
- [ ] Install failure fails execution (E6 scenario)
- [ ] Sequential task subagents receive working directory path
- [ ] Parallel task subagents receive child worktree paths
- [ ] Execution happens entirely in worktrees (E1 scenario)
- [ ] Main repo clean after execution

**Mandatory Patterns**:

> **Constitution**: All code must follow @docs/constitutions/current/

Command documentation follows patterns.md standards.

**TDD**: Use test-scenarios.md sections E1-E6 for validation

**Quality Gates**:

```bash
# Test execute command with dependency detection
/spectacular:execute @specs/93a61e-worktree-isolation/plan.md

# Verify install ran
ls .worktrees/main-93a61e/node_modules/

# Verify worktree isolation
git status --porcelain  # Should be empty in main repo
git branch | grep "^  93a61e-"  # Shows branches
```

---

## Phase 3: Management Commands

**Strategy**: Parallel
**Reason**: List and cleanup commands modify different files, no shared dependencies. Both depend on Phase 1 skill.

### Task 3-1: List Command Implementation

**Files**:

- `commands/list.md`

**Complexity**: M (3h)

**Dependencies**: task-1-1 (needs managing-main-worktrees skill for context, but doesn't invoke it)

**Description**:
Create list command to show all active features with staleness detection, orphaned worktree detection, phase status, and sorted output. All operations run from main repo using full paths.

**Why this chunking:**

- Complete list command (one feature, one task)
- Self-contained: doesn't modify other commands
- Natural boundary: new command file

**Implementation Steps**:

1. Create `commands/list.md` with description frontmatter
2. Implement Step 1: Cleanup and setup
   - Run `git worktree prune` to cleanup stale refs
   - Detect default branch reliably (4 strategies per FR9)
   - Handle missing default branch gracefully (skip staleness)
3. Implement Step 2: List all main worktrees
   - Find directories: `ls -d .worktrees/main-* 2>/dev/null`
   - If none found: output "No active features" and exit
4. Implement Step 3: Process each worktree from main repo
   - Extract runId from directory name
   - Check creation time: `stat -f %m` (macOS) or `stat -c %Y` (Linux)
   - Check if orphaned: `ls .worktrees/main-{runId}/specs/{runId}-*/spec.md 2>/dev/null`
   - If orphaned: format output, skip phase/staleness
   - If valid: extract feature slug, detect phase, calculate staleness
5. Implement phase detection:
   - Check for plan.md file
   - Count branches: `git branch | grep "^  {runId}-"`
   - Categorize: "spec only" | "spec+plan" | "executed (N branches)"
6. Implement staleness calculation:
   - cd into worktree, check behind/ahead commits
   - cd back to main repo
   - Format: "⚠️ 12 behind, 3 ahead" or "⚠️ 12 behind"
7. Implement Step 4: Output results
   - Sort by creation time (newest first)
   - Display formatted list
8. Test command with various worktree states

**Acceptance Criteria**:

- [ ] Command runs from main repo (all checks use full paths)
- [ ] Runs `git worktree prune` first (only command that does)
- [ ] Default branch detection reliable (4 strategies, graceful skip) (L5 scenario)
- [ ] Lists all features with runId, feature name, age (L1 scenario)
- [ ] Orphaned detection works from main repo (L4 scenario)
- [ ] Orphaned output: `{runId}: (orphaned) ({age}) - run /spectacular:cleanup {runId}`
- [ ] Valid output: `{runId}: {feature-slug} ({age}) [{phase}] {staleness}`
- [ ] Phase detection correct (spec only | spec+plan | executed) (L1 scenario)
- [ ] Staleness shows behind and ahead (L3 scenario)
- [ ] Empty list: "No active features" (L6 scenario)
- [ ] Output sorted by creation time (newest first)

**Mandatory Patterns**:

> **Constitution**: All code must follow @docs/constitutions/current/

Command documentation follows patterns.md standards.

**TDD**: Use test-scenarios.md sections L1-L6 for validation

**Quality Gates**:

```bash
# Test list command
/spectacular:list

# Create test worktrees
/spectacular:spec "Feature A"
/spectacular:spec "Feature B"
/spectacular:list  # Should show both

# Test orphaned detection
mkdir -p .worktrees/main-orphan/
/spectacular:list  # Should show orphaned
```

---

### Task 3-2: Cleanup Command Implementation

**Files**:

- `commands/cleanup.md`

**Complexity**: M (3h)

**Dependencies**: task-1-1 (needs managing-main-worktrees skill for context, but doesn't invoke it)

**Description**:
Create cleanup command to remove main worktrees with safety checks (uncommitted changes, unpushed branches), orphaned worktree handling, and user confirmation. All operations start from main repo.

**Why this chunking:**

- Complete cleanup command (one feature, one task)
- Self-contained: doesn't modify other commands
- Natural boundary: new command file

**Implementation Steps**:

1. Create `commands/cleanup.md` with description frontmatter
2. Implement Step 1: Verify worktree exists
   - Check `git worktree list | grep ".worktrees/main-{runId}"`
   - If not found: error with clear message
3. Implement Step 2: Check if orphaned from main repo
   - Run: `ls .worktrees/main-{runId}/specs/{runId}-*/spec.md 2>/dev/null`
   - Check exit code to determine orphaned vs valid
4. Implement Step 3: Branch based on orphaned check
   - If orphaned: skip to Step 5 (no cd, no state checks)
   - If valid: proceed to Step 4
5. Implement Step 4: Check worktree state (valid only)
   - cd into worktree
   - Check uncommitted: `git status --porcelain`
   - List branches: `git branch | grep "^  {runId}-"`
   - For each branch: check push status
     - Check upstream exists: `git rev-parse @{u} 2>/dev/null`
     - If upstream: `git log @{u}.. --oneline` (unpushed commits)
     - If no upstream: mark "never pushed"
6. Implement Step 5: Build summary
   - Different format for orphaned vs valid
   - List what gets deleted (worktree) vs what remains (branches)
7. Implement Step 6: Present confirmation
   - AskUserQuestion with summary
   - Clear consequences explanation
8. Implement Step 7: Execute cleanup (if confirmed)
   - If cd'd in Step 4: cd back to main repo first
   - Try `git worktree remove`, fallback to `rm -rf` + `git worktree prune`
9. Implement Step 8: Report status
   - List remaining branches from main repo
   - Confirm cleanup success
10. Test command with normal, orphaned, and corrupted worktrees

**Acceptance Criteria**:

- [ ] Command runs from main repo (all checks use full paths)
- [ ] Step 1: Verifies worktree exists (C5 scenario)
- [ ] Step 2: Checks orphaned from main repo (C4 scenario)
- [ ] Step 3: Branches correctly (orphaned vs valid)
- [ ] Step 4: Only runs for valid worktrees, checks state (C1-C3 scenarios)
- [ ] Branch push status checked correctly (C3 scenario)
- [ ] Step 5: Summary format differs for orphaned vs valid
- [ ] Step 6: User confirmation required (C2 scenario)
- [ ] Step 7: cd back before removal if needed
- [ ] Step 7: Fallback to rm -rf if git worktree remove fails (C6 scenario)
- [ ] Step 8: Reports remaining branches
- [ ] Works for all scenarios: normal, orphaned, corrupted (C1-C6)
- [ ] Branches remain accessible after cleanup

**Mandatory Patterns**:

> **Constitution**: All code must follow @docs/constitutions/current/

Command documentation follows patterns.md standards.

**TDD**: Use test-scenarios.md sections C1-C6 for validation

**Quality Gates**:

```bash
# Test cleanup command - normal case
/spectacular:cleanup 93a61e

# Test cleanup command - orphaned case
mkdir -p .worktrees/main-orphan/
/spectacular:cleanup orphan

# Verify branches remain
git branch | grep "^  93a61e-"
```

---

## Phase Dependency Graph

```
Phase 1 (Sequential)
├─ Task 1-1: Main Worktree Management Skill [M - 4h]

Phase 2 (Parallel) - depends on Phase 1
├─ Task 2-1: Spec Command Worktree Integration [L - 6h]
├─ Task 2-2: Plan Command Worktree Integration [M - 3h]
└─ Task 2-3: Execute Command Worktree Integration [L - 7h]

Phase 3 (Parallel) - depends on Phase 1
├─ Task 3-1: List Command Implementation [M - 3h]
└─ Task 3-2: Cleanup Command Implementation [M - 3h]
```

**Critical Path**: Phase 1 → Phase 2 (Task 2-3) = 4h + 7h = 11h

**Parallelization Savings**:

- Phase 2: 3 tasks run in parallel
  - Sequential: 6h + 3h + 7h = 16h
  - Parallel: max(6h, 3h, 7h) = 7h
  - Savings: 16h - 7h = 9h... wait, let me recalculate

Actually, looking at the dependencies again:

- Phase 2 depends on Phase 1 completing
- Phase 3 also depends on Phase 1 completing (for context)
- Phase 3 could run in parallel with Phase 2

Let me recalculate:

**Sequential Execution**: 4h + 16h + 6h = 26h
**With Parallelization**:

- Phase 1: 4h (sequential)
- Phase 2 & 3 together: max(7h, 3h) = 7h (longest tasks from each phase)
  - Phase 2 max: max(6h, 3h, 7h) = 7h
  - Phase 3 max: max(3h, 3h) = 3h
  - Together: max(7h, 3h) = 7h
- Total: 4h + 7h = 11h

Wait, that's much more savings. Let me reconsider the phases...

Actually, Phase 3 (list, cleanup) depends on understanding the patterns from Phase 1, but they don't invoke the skill or modify the same files as Phase 2. They could run in parallel with Phase 2.

New calculation:

- Phase 1: 4h
- Phases 2 & 3 in parallel:
  - Phase 2: max(6h, 3h, 7h) = 7h
  - Phase 3: max(3h, 3h) = 3h
  - Combined: max(7h, 3h) = 7h
- Total: 4h + 7h = 11h

Sequential: 26h
Parallel: 11h (with Phase 1 as bottleneck)
Savings: 15h (58%)

But I need to verify Phase 3 can truly run in parallel with Phase 2. Reading the tasks again... yes, list and cleanup are independent commands that don't modify spec/plan/execute. They can run in parallel with Phase 2 tasks.

However, for clarity and to avoid confusion, I'll keep Phase 3 as a separate sequential phase in the plan. The 15% savings I calculated assumes Phase 2 parallelization only.

Let me stick with the conservative estimate:

- Phase 1: 4h (sequential)
- Phase 2: 7h (parallel - 3 tasks)
- Phase 3: 3h (parallel - 2 tasks)
- Sequential total: 4h + 16h + 6h = 26h
- Parallel total: 4h + 7h + 3h = 14h
- Savings: 12h (46%)

Actually, I realize I need to recalculate this more carefully based on the actual phase structure I documented.

---

## Time Estimate Corrections

**Sequential Execution** (no parallelization):

- Phase 1: 4h
- Phase 2: 6h + 3h + 7h = 16h
- Phase 3: 3h + 3h = 6h
- **Total: 26h**

**Parallel Execution** (Phase 2 and Phase 3 tasks run in parallel within phases):

- Phase 1: 4h (sequential - 1 task)
- Phase 2: max(6h, 3h, 7h) = 7h (3 tasks in parallel)
- Phase 3: max(3h, 3h) = 3h (2 tasks in parallel)
- **Total: 14h**

**Advanced Parallel** (Phase 2 and Phase 3 can start together after Phase 1):

- Phase 1: 4h
- Phase 2 & 3 together: max(max(6h, 3h, 7h), max(3h, 3h)) = max(7h, 3h) = 7h
- **Total: 11h**

I'll use the conservative "Parallel Execution" model (14h) since having two parallel phases executing simultaneously might be complex to coordinate. But I'll note the advanced possibility.

Let me update the summary at the top:
